{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio Denoiser.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORqh0W6ko3U"
      },
      "source": [
        "# Import a bunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IthQX_B8WMXG",
        "outputId": "2250bc6e-b358-4dc6-aaab-ea3d04740176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "!pip install pydub\n",
        "#!pip install librosa==0.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBhDKGl1h90p"
      },
      "source": [
        "# Import a bunch\n",
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import os\n",
        "from glob import glob\n",
        "import shutil\n",
        "import pathlib\n",
        "import random\n",
        "import pickle\n",
        "from IPython.display import Audio\n",
        "import pydub\n",
        "\n",
        "# Model\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import backend \n",
        "from tensorflow.keras import initializers\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIprw9gmlmzX",
        "outputId": "b0787168-bc41-494b-f5ed-55ced104e9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDHLMOFkkPs"
      },
      "source": [
        "# Extract data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk9lyjjxiKFU",
        "outputId": "5825f487-7960-4fa1-aa56-0b157c480fa4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Install kaggle\n",
        "! pip install -q kaggle\n",
        "#Upload json file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-244d08e8-39e3-4d23-81d2-8690b6e439d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-244d08e8-39e3-4d23-81d2-8690b6e439d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"datnguyn\",\"key\":\"69c14f649dd7364914db7810156aeea6\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km9CWj-7iLfh",
        "outputId": "1591e907-ab12-43fc-aa12-eb4b8dfcdeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Download dataset\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d mozillaorg/common-voice\n",
        "! kaggle datasets download -d chrisfilo/urbansound8k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading common-voice.zip to /content\n",
            "100% 12.0G/12.0G [04:52<00:00, 45.9MB/s]\n",
            "100% 12.0G/12.0G [04:52<00:00, 44.1MB/s]\n",
            "Downloading urbansound8k.zip to /content\n",
            "100% 5.61G/5.61G [02:27<00:00, 48.4MB/s]\n",
            "100% 5.61G/5.61G [02:27<00:00, 40.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5zoxo8eiiqe"
      },
      "source": [
        "# Extract dataset\n",
        "# Common Voice\n",
        "%mkdir /content/common_voice\n",
        "%cd /content/common_voice\n",
        "!unzip /content/common-voice.zip && rm /content/common-voice.zip\n",
        "# Urban Sound\n",
        "%mkdir /content/urban_sound\n",
        "%cd /content/urban_sound\n",
        "!unzip /content/urbansound8k.zip && rm /content/urbansound8k.zip\n",
        "\n",
        "#Return to main directory\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjhOR-GrlDiC"
      },
      "source": [
        "# Functions definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez9ErimAlHUn"
      },
      "source": [
        "#Utils\n",
        "\n",
        "# Convert from .mp3 files to .wav files\n",
        "def convert_to_wav(path, destination_path):\n",
        "  filenames = os.path.split(path)[-1]\n",
        "  # convert wav to mp3                                                            \n",
        "  sound = pydub.AudioSegment.from_mp3(path)\n",
        "  sound.export(os.path.join(destination_path, str(filenames[:-4]+'.wav')), format=\"wav\")\n",
        "  os.remove(path)\n",
        "# Read .wav files\n",
        "def read_wav(path, sr=8000):\n",
        "  signal, sr = librosa.load(path, sr=8000, mono=True)\n",
        "  return signal\n",
        "# Trim silence audio\n",
        "#def trim_audio(audio):\n",
        "#  signal, sr = librosa.effects.trim(audio, top_db=10)\n",
        "#  return signal\n",
        "# Combine clean audio and noisy audio\n",
        "def get_noisy_audio(clean_audio, noise_signal):\n",
        "  if len(clean_audio) >= len(noise_signal):\n",
        "    while len(clean_audio) >= len(noise_signal):\n",
        "        noise_signal = np.append(noise_signal, noise_signal)\n",
        "  ## Extract a noise segment from a random location in the noise file\n",
        "  ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n",
        "  noiseSegment = noise_signal[ind: ind + clean_audio.size]\n",
        "  speech_power = np.sum(clean_audio ** 2)\n",
        "  noise_power = np.sum(noiseSegment ** 2)\n",
        "  noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n",
        "  return noisyAudio\n",
        "# Extended short sound\n",
        "#def extend_short_sounds(data, target_duration=24000):\n",
        "#  stretched = librosa.effects.time_stretch(data, rate=data.shape[0]/target_duration)\n",
        "#  return stretched[:target_duration:]\n",
        "\n",
        "# Load pickle file\n",
        "def load_data(pikcle_path):\n",
        "  with open(pikcle_path, \"rb\") as fp: # opening for reading\n",
        "    data = pickle.load(fp)\n",
        "  # convert list into numpy arrays\n",
        "  # inputs = np.array(data['train_data_noisy'])\n",
        "  # targets = np.array(data['train_data'])\n",
        "  return data\n",
        "# Save data to pickle file\n",
        "def save_pickle(pickle_path, data_dict):\n",
        "  with open(pickle_path, \"wb\") as fp: # opening for reading\n",
        "    pickle.dump(data_dict, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbiN0DxolIcW"
      },
      "source": [
        "# Get spectrogram and STFT\n",
        "def get_stft(signal, n_fft=255, hop_length=188):\n",
        "    return librosa.stft(signal, n_fft=n_fft, hop_length=hop_length, center=True)\n",
        "\n",
        "def data_to_spec(signal):\n",
        "    if signal.shape[0] > 24000:\n",
        "      idx = np.random.randint(0, signal.shape[0] - 24000) #Slide the signal to 3s length\n",
        "      signal = signal[idx: idx + 24000]\n",
        "      stft = get_stft(signal)\n",
        "      spec, phase = librosa.magphase(stft)\n",
        "      spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
        "      return spec, phase\n",
        "    elif signal.shape[0] < 24000: #Ignore short signal under 3s length\n",
        "      pass\n",
        "\n",
        "def file_to_spec(path):\n",
        "    signal = read_wav(path)\n",
        "    #if signal.shape[0] > 24000:\n",
        "    #  idx = np.random.randint(0, signal.shape[0] - 24000) #Avoid keeping the silence audio at the beginning\n",
        "    #  signal = signal[idx: idx + 24000]\n",
        "    return data_to_spec(signal)\n",
        "    #elif signal.shape[0] < 24000:\n",
        "    #  pass\n",
        "\n",
        "def get_audio_from_stft(stft_features):\n",
        "    return librosa.griffinlim(stft_features)\n",
        "\n",
        "def get_audio_from_spec(spectrogram, phase, hop_length=188, length=24000):\n",
        "  D = librosa.db_to_amplitude(spectrogram, ref=1)\n",
        "  D_re = D*phase\n",
        "  audio_reconstruct = librosa.core.istft(D_re, hop_length=hop_length, length=length)\n",
        "  return Audio(audio_reconstruct, rate=8000)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCRSI2zqlLeH"
      },
      "source": [
        "# Get MFCC\n",
        "def data_to_mfcc(data):\n",
        "    return librosa.feature.mfcc(data) \n",
        "\n",
        "def file_to_mfcc(path):\n",
        "    data, rate = librosa.load(path)\n",
        "    if data.shape[0] != TARGET_LEN: # handle short sounds\n",
        "        data = extend_short_sounds(data)\n",
        "    return data_to_mfcc(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpXVjh1m0-05"
      },
      "source": [
        "#norm_value = 2 * data ./ (max_range_value - min_range_value);\n",
        "def scaled_in(matrix_spec):\n",
        "    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec + 46)/50\n",
        "    return matrix_spec\n",
        "\n",
        "def scaled_ou(matrix_spec):\n",
        "    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n",
        "    matrix_spec = (matrix_spec -6 )/82\n",
        "    return matrix_spec\n",
        "\n",
        "def inv_scaled_in(matrix_spec):\n",
        "    \"inverse global scaling apply to noisy voices spectrograms\"\n",
        "    matrix_spec = matrix_spec * 50 - 46\n",
        "    return matrix_spec\n",
        "\n",
        "def inv_scaled_ou(matrix_spec):\n",
        "    \"inverse global scaling apply to noise models spectrograms\"\n",
        "    matrix_spec = matrix_spec * 82 + 6\n",
        "    return matrix_spec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZGlJpt_aiiU"
      },
      "source": [
        "# Prediction - Input preprocessing\n",
        "def data_to_spec_auto_scale(audio_path):\n",
        "    signal = read_wav(audio_path)\n",
        "    frame_length = signal.shape[0]\n",
        "    hop_length = math.floor(frame_length/127)\n",
        "    stft = librosa.stft(signal, n_fft=255, hop_length=hop_length, center=True)\n",
        "    if stft.shape[1] == 128:\n",
        "      spec, phase = librosa.magphase(stft)\n",
        "      spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
        "      return spec, phase, hop_length, frame_length\n",
        "    else:\n",
        "      hop_length = hop_length-1\n",
        "      stft = librosa.stft(signal, n_fft=255, hop_length=hop_length, center=True)\n",
        "      spec, phase = librosa.magphase(stft)\n",
        "      spec = librosa.amplitude_to_db(spec, ref=1)\n",
        "      return spec, phase, hop_length, frame_length   \n",
        "\n",
        "# Prediction\n",
        "def predict(audio_path):\n",
        "    print(os.getcwd())\n",
        "    model = keras.models.load_model('./model.h5')\n",
        "    a, b, c, d = data_to_spec_auto_scale(audio_path)\n",
        "    a = a[np.newaxis, ..., np.newaxis]\n",
        "    m = model.predict(a)\n",
        "    m_ou = a - m\n",
        "    n_ou = np.squeeze(m_ou, (0, 3))\n",
        "    n_ou = magnitude_db_and_phase_to_audio(n_ou, b, d, c)\n",
        "    return n_ou\n",
        "\n",
        "# Output preprocessing\n",
        "def signal_to_audio(spec, phase, frame_length, hop_length_fft):\n",
        "    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n",
        "\n",
        "    spec_to_amplitude = librosa.db_to_amplitude(spec, ref=1.0)\n",
        "\n",
        "    # taking magnitude and phase of audio\n",
        "    denoised_signal = spec_to_amplitude * phase\n",
        "    denoised_audio = librosa.core.istft(denoised_signal, hop_length=hop_length_fft, length=frame_length)\n",
        "\n",
        "    return Audio(denoised_audio, rate=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ssw9E5qlMZ1"
      },
      "source": [
        "# Preparing STFT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pGvPYDSi6_P",
        "outputId": "5e8f1605-7fb3-44b3-d1a1-5211993b837b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Getting list of mp3 files\n",
        "mp3_data_path = pathlib.Path('/content/common_voice/cv-valid-train/cv-valid-train')\n",
        "mp3_audio_paths = [os.path.join(mp3_data_path, name) for name in os.listdir(mp3_data_path) if os.path.isfile(os.path.join(mp3_data_path, name))]\n",
        "len(mp3_audio_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vteN27W2_MpJ"
      },
      "source": [
        "#Remore redundant directories\n",
        "\n",
        "#shutil.rmtree('/content/common_voice/cv-other-dev')\n",
        "#shutil.rmtree('/content/common_voice/cv-invalid')\n",
        "#shutil.rmtree('/content/common_voice/cv-other-test')\n",
        "#shutil.rmtree('/content/common_voice/cv-other-train')\n",
        "#shutil.rmtree('/content/common_voice/cv-valid-dev')\n",
        "#shutil.rmtree('/content/common_voice/cv-valid-test')\n",
        "#shutil.rmtree('/content/clean')\n",
        "#random.shuffle(mp3_audio_paths)\n",
        "#shutil.rmtree('/content/common_voice')\n",
        "#shutil.rmtree('/content/urban_sound')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCYSkclIlvTv"
      },
      "source": [
        "# Converting Common Voice .mp3 files to .wav\n",
        "dest = '/content/clean'\n",
        "for i in range(10001,15001):\n",
        "  count = 0\n",
        "  try:\n",
        "    if i % 500 ==0:\n",
        "      print('processed {}'.format(i))\n",
        "    convert_to_wav(mp3_audio_paths[i], dest)\n",
        "  except:\n",
        "    count +=1\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHj902yZlypR",
        "outputId": "8a993918-166c-417a-8947-384fdf7b97d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# New .wav files\n",
        "data_path = pathlib.Path('/content/clean')\n",
        "all_train_paths = [os.path.join(data_path, name) for name in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, name))]\n",
        "len(all_train_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWxLS8LjMLQ",
        "outputId": "b7e67300-3843-4050-c33e-44baf5ed12fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Move all noise files into one folder\n",
        "src_noise_files = glob(os.path.join('/content/urban_sound', '*', '*.wav'))\n",
        "noise_dir = pathlib.Path('/content/noises')\n",
        "for wav_file in src_noise_files:\n",
        "  shutil.move((wav_file),(noise_dir))\n",
        "\n",
        "# Getting list of noise files\n",
        "all_noise_path = glob(os.path.join(noise_dir, '*.wav'))\n",
        "len(all_noise_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8732"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd3j3VmxoR6R"
      },
      "source": [
        "# Shuffle the list of files\n",
        "random.shuffle(all_train_paths)\n",
        "random.shuffle(all_noise_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I420AL3MjXhc"
      },
      "source": [
        "# Get signal of audio and noisy audio\n",
        "train_audio = []\n",
        "train_audio_noisy = []\n",
        "count = 0\n",
        "count_success = 0\n",
        "for path in all_train_paths:\n",
        "  \n",
        "  clean_signal = read_wav(path)\n",
        "  noise_file = random.choice(all_noise_path)\n",
        "  noise_audio = read_wav(noise_file)\n",
        "  noisy_signal = get_noisy_audio(clean_signal, noise_audio)\n",
        "  if clean_signal.shape[0] == noisy_signal.shape[0]:\n",
        "    train_audio.append(clean_signal)\n",
        "    train_audio_noisy.append(noisy_signal)\n",
        "    count_success += 1\n",
        "    if count_success % 100 == 0:\n",
        "      print('Processed {}'.format(count_success))\n",
        "    \n",
        "  else:\n",
        "    count += 1\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG7Og-dNjZ3L"
      },
      "source": [
        "# Get spectrogram of clean and noisy data\n",
        "data={\n",
        "    'clean_train_data': [],\n",
        "    'noisy_train_data': []\n",
        "}\n",
        "for i in range(len(train_audio)):\n",
        "  try:\n",
        "    clean_spec, clean_phase = data_to_spec(train_audio[i])\n",
        "    noisy_spec, noisy_phase = data_to_spec(train_audio_noisy[i])\n",
        "    data['clean_train_data'].append(clean_spec)\n",
        "    data['noisy_train_data'].append(noisy_spec)\n",
        "  except:\n",
        "    print('Skip file: {}'.format(i))\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8MOQeAeYaNG"
      },
      "source": [
        "# Save to pickle file for further uses\n",
        "save_pickle('/content/drive/My Drive/train_audio.pkl', data['clean_train_data'])\n",
        "save_pickle('/content/drive/My Drive/train_audio_noisy.pkl',data['noisy_train_data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zUZ8gY8PFFK",
        "outputId": "d7b655c5-933f-4379-86a9-fe1b7e02ccef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Available file for training: {} files'.format(len(data['clean_train_data'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available file for training: 11618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iteo2EEHhYDG"
      },
      "source": [
        "X = np.array(data['noisy_train_data'])\n",
        "y = np.array(data['clean_train_data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Xr5BMXVhru"
      },
      "source": [
        "y_re = X-y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkbqWbq6PcA5"
      },
      "source": [
        "X = X[..., np.newaxis]\n",
        "y = y[..., np.newaxis]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OAaCjrvWCpk",
        "outputId": "20ef1125-4d66-46d0-9e11-b7268d3efc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11618, 128, 128, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0crnFu5qYgH4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUzOpgIODAAb"
      },
      "source": [
        "# Load data from pickle file\n",
        "X = load_data('/content/drive/My Drive/data/full_noise.pkl')\n",
        "y = load_data('/content/drive/My Drive/data/full_train.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_R6T0_mA1o_",
        "outputId": "a8c3335c-6794-4c09-e943-3525dc6390b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Available for train: {} files'.format(X.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available for train: 48082 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8reiRU77O66",
        "outputId": "cc6148e3-7b50-4cdb-f969-30635af2ba45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Define UNet\n",
        "def unet(pretrained_weights = None,input_size = (128,128,1)):\n",
        "    size_filter_in = 16\n",
        "    #kernel_init = 'glorot_uniform'\n",
        "    #kernel_init = tf.keras.initializers.glorot_normal\n",
        "    activation_layer = None \n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(inputs)\n",
        "    conv1 = LeakyReLU()(conv1)\n",
        "    conv1 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv1)\n",
        "    conv1 = LeakyReLU()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(pool1)\n",
        "    conv2 = LeakyReLU()(conv2)\n",
        "    conv2 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv2)\n",
        "    conv2 = LeakyReLU()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(pool2)\n",
        "    conv3 = LeakyReLU()(conv3)\n",
        "    conv3 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv3)\n",
        "    conv3 = LeakyReLU()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(pool3)\n",
        "    conv4 = LeakyReLU()(conv4)\n",
        "    conv4 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv4)\n",
        "    conv4 = LeakyReLU()(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(pool4)\n",
        "    conv5 = LeakyReLU()(conv5)\n",
        "    conv5 = Conv2D(size_filter_in*16, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv5)\n",
        "    conv5 = LeakyReLU()(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(size_filter_in*8, 2, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(UpSampling2D(size = (2,2))(drop5))\n",
        "    up6 = LeakyReLU()(up6)\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(merge6)\n",
        "    conv6 = LeakyReLU()(conv6)\n",
        "    conv6 = Conv2D(size_filter_in*8, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv6)\n",
        "    conv6 = LeakyReLU()(conv6)\n",
        "    up7 = Conv2D(size_filter_in*4, 2, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(UpSampling2D(size = (2,2))(conv6))\n",
        "    up7 = LeakyReLU()(up7)\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(merge7)\n",
        "    conv7 = LeakyReLU()(conv7)\n",
        "    conv7 = Conv2D(size_filter_in*4, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv7)\n",
        "    conv7 = LeakyReLU()(conv7)\n",
        "    up8 = Conv2D(size_filter_in*2, 2, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LeakyReLU()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(merge8)\n",
        "    conv8 = LeakyReLU()(conv8)\n",
        "    conv8 = Conv2D(size_filter_in*2, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv8)\n",
        "    conv8 = LeakyReLU()(conv8)\n",
        "    \n",
        "    up9 = Conv2D(size_filter_in, 2, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(UpSampling2D(size = (2,2))(conv8))\n",
        "    up9 = LeakyReLU()(up9)\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(merge9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv9 = Conv2D(size_filter_in, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = activation_layer, padding = 'same', kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                        bias_initializer=tf.keras.initializers.zeros())(conv9)\n",
        "    conv9 = LeakyReLU()(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'tanh')(conv9)\n",
        "\n",
        "    model = Model(inputs,conv10)\n",
        "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=5e-6), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVmDOiih6gGC"
      },
      "source": [
        "#Creating checkpoints\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='new_model_{epoch}.h5',\n",
        "    save_best_only = True,\n",
        "    verbose=1)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience =30,restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2z1DYlVaPP"
      },
      "source": [
        "# Define inputs and outputs\n",
        "X_in = X\n",
        "X_ou = y\n",
        "X_ou = X_in - X_ou\n",
        "X_in = scaled_in(X_in)\n",
        "X_ou = scaled_ou(X_ou)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTyJ-h_mV0kk"
      },
      "source": [
        "#Reshape the inputs and outputs for training\n",
        "X_in = X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
        "X_ou = X_ou.reshape(X_ou.shape[0],X_ou.shape[1],X_ou.shape[2],1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_in, X_ou, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzjNQIGnW2-c",
        "outputId": "62e818f2-fb6a-4e4b-be28-0027f316da8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generator_nn=unet()\n",
        "generator_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 128, 128, 16) 160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 128, 128, 16) 0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 128, 128, 16) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 64, 64, 32)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 64, 64, 32)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 128)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 128)  0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 16, 16, 128)  0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 8, 8, 256)    0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 256)    0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 8, 8, 256)    0           leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  131200      up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16, 16, 256)  0           dropout[0][0]                    \n",
            "                                                                 leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  147584      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 128)  0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 64)   32832       up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           leaky_re_lu_5[0][0]              \n",
            "                                                                 leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 64)   36928       leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 64)   0           leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 64, 64, 32)   8224        up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 64)   0           leaky_re_lu_3[0][0]              \n",
            "                                                                 leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 64, 64, 32)   9248        leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 32) 0           leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 16) 2064        up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 128, 32) 0           leaky_re_lu_1[0][0]              \n",
            "                                                                 leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 128, 128, 16) 2320        leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 128, 128, 2)  290         leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 128, 128, 2)  0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 128, 128, 1)  3           leaky_re_lu_22[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,941,093\n",
            "Trainable params: 1,941,093\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzYcptaYQXsp"
      },
      "source": [
        "# New Section 4.47"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNM4HOnyQXJP",
        "outputId": "d9c11cfc-b0fe-4005-9d4e-99fc3e2f6298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = generator_nn.fit(X_in, X_ou, epochs=100, \n",
        "                               batch_size=32, \n",
        "                               shuffle=True, \n",
        "                               callbacks=[checkpoint_cb,early_stopping_cb], \n",
        "                               verbose=1, \n",
        "                               validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11618, 128, 128, 1)\n",
            "(11618, 128, 128, 1)\n",
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 128, 128, 16) 160         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_69 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 128, 128, 16) 2320        leaky_re_lu_69[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_70 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 64, 64, 16)   0           leaky_re_lu_70[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 64, 64, 32)   4640        max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 64, 64, 32)   9248        leaky_re_lu_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_72 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 32)   0           leaky_re_lu_72[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 32, 32, 64)   18496       max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_73 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 32, 32, 64)   36928       leaky_re_lu_73[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_74 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 64)   0           leaky_re_lu_74[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 128)  73856       max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_75 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 128)  147584      leaky_re_lu_75[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_76 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 16, 16, 128)  0           leaky_re_lu_76[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 8, 8, 128)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 256)    295168      max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_77 (LeakyReLU)      (None, 8, 8, 256)    0           conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 256)    590080      leaky_re_lu_77[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_78 (LeakyReLU)      (None, 8, 8, 256)    0           conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 8, 8, 256)    0           leaky_re_lu_78[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_12 (UpSampling2D) (None, 16, 16, 256)  0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 128)  131200      up_sampling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_79 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 256)  0           dropout_6[0][0]                  \n",
            "                                                                 leaky_re_lu_79[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 16, 16, 128)  295040      concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_80 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 16, 16, 128)  147584      leaky_re_lu_80[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_81 (LeakyReLU)      (None, 16, 16, 128)  0           conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_13 (UpSampling2D) (None, 32, 32, 128)  0           leaky_re_lu_81[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 32, 32, 64)   32832       up_sampling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_82 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 32, 32, 128)  0           leaky_re_lu_74[0][0]             \n",
            "                                                                 leaky_re_lu_82[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_83 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 32, 32, 64)   36928       leaky_re_lu_83[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_84 (LeakyReLU)      (None, 32, 32, 64)   0           conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling2D) (None, 64, 64, 64)   0           leaky_re_lu_84[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 64, 64, 32)   8224        up_sampling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_85 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 64, 64, 64)   0           leaky_re_lu_72[0][0]             \n",
            "                                                                 leaky_re_lu_85[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_86 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 64, 64, 32)   9248        leaky_re_lu_86[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_87 (LeakyReLU)      (None, 64, 64, 32)   0           conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling2D) (None, 128, 128, 32) 0           leaky_re_lu_87[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 128, 128, 16) 2064        up_sampling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_88 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 128, 128, 32) 0           leaky_re_lu_70[0][0]             \n",
            "                                                                 leaky_re_lu_88[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_89 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 128, 128, 16) 2320        leaky_re_lu_89[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_90 (LeakyReLU)      (None, 128, 128, 16) 0           conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 128, 128, 2)  290         leaky_re_lu_90[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_91 (LeakyReLU)      (None, 128, 128, 2)  0           conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 128, 128, 1)  3           leaky_re_lu_91[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 1,941,093\n",
            "Trainable params: 1,941,093\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 2.2788 - mae: 19.9672\n",
            "Epoch 00001: val_loss improved from inf to 1.68164, saving model to mymodel_1.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 2.2786 - mae: 19.9673 - val_loss: 1.6816 - val_mae: 19.9369\n",
            "Epoch 2/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 1.4581 - mae: 19.9182\n",
            "Epoch 00002: val_loss improved from 1.68164 to 1.37387, saving model to mymodel_2.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 1.4580 - mae: 19.9194 - val_loss: 1.3739 - val_mae: 19.9949\n",
            "Epoch 3/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 1.1284 - mae: 19.9189\n",
            "Epoch 00003: val_loss did not improve from 1.37387\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 1.1284 - mae: 19.9189 - val_loss: 1.8360 - val_mae: 20.0682\n",
            "Epoch 4/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.9644 - mae: 19.9203\n",
            "Epoch 00004: val_loss did not improve from 1.37387\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.9644 - mae: 19.9203 - val_loss: 1.5192 - val_mae: 20.0518\n",
            "Epoch 5/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.8896 - mae: 19.9225\n",
            "Epoch 00005: val_loss did not improve from 1.37387\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.8896 - mae: 19.9219 - val_loss: 1.5152 - val_mae: 20.0570\n",
            "Epoch 6/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.8365 - mae: 19.9230\n",
            "Epoch 00006: val_loss did not improve from 1.37387\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.8365 - mae: 19.9233 - val_loss: 1.3806 - val_mae: 20.0505\n",
            "Epoch 7/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.7941 - mae: 19.9251\n",
            "Epoch 00007: val_loss did not improve from 1.37387\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.7941 - mae: 19.9244 - val_loss: 1.4021 - val_mae: 20.0561\n",
            "Epoch 8/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.7584 - mae: 19.9260\n",
            "Epoch 00008: val_loss improved from 1.37387 to 1.26565, saving model to mymodel_8.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.7583 - mae: 19.9260 - val_loss: 1.2656 - val_mae: 20.0484\n",
            "Epoch 9/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.7288 - mae: 19.9280\n",
            "Epoch 00009: val_loss improved from 1.26565 to 1.18391, saving model to mymodel_9.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.7288 - mae: 19.9274 - val_loss: 1.1839 - val_mae: 20.0446\n",
            "Epoch 10/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.7017 - mae: 19.9287\n",
            "Epoch 00010: val_loss improved from 1.18391 to 0.99769, saving model to mymodel_10.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.7016 - mae: 19.9291 - val_loss: 0.9977 - val_mae: 20.0297\n",
            "Epoch 11/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.6804 - mae: 19.9307\n",
            "Epoch 00011: val_loss improved from 0.99769 to 0.91945, saving model to mymodel_11.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.6803 - mae: 19.9308 - val_loss: 0.9194 - val_mae: 20.0240\n",
            "Epoch 12/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.6590 - mae: 19.9332\n",
            "Epoch 00012: val_loss did not improve from 0.91945\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.6590 - mae: 19.9328 - val_loss: 1.0601 - val_mae: 20.0427\n",
            "Epoch 13/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.6402 - mae: 19.9356\n",
            "Epoch 00013: val_loss did not improve from 0.91945\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.6402 - mae: 19.9355 - val_loss: 0.9466 - val_mae: 20.0342\n",
            "Epoch 14/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.6248 - mae: 19.9386\n",
            "Epoch 00014: val_loss did not improve from 0.91945\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.6248 - mae: 19.9376 - val_loss: 0.9806 - val_mae: 20.0406\n",
            "Epoch 15/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.6109 - mae: 19.9399\n",
            "Epoch 00015: val_loss improved from 0.91945 to 0.86081, saving model to mymodel_15.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.6110 - mae: 19.9399 - val_loss: 0.8608 - val_mae: 20.0304\n",
            "Epoch 16/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5984 - mae: 19.9419\n",
            "Epoch 00016: val_loss improved from 0.86081 to 0.84054, saving model to mymodel_16.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.5984 - mae: 19.9421 - val_loss: 0.8405 - val_mae: 20.0307\n",
            "Epoch 17/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5876 - mae: 19.9437\n",
            "Epoch 00017: val_loss improved from 0.84054 to 0.83133, saving model to mymodel_17.h5\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.5877 - mae: 19.9440 - val_loss: 0.8313 - val_mae: 20.0322\n",
            "Epoch 18/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5776 - mae: 19.9465\n",
            "Epoch 00018: val_loss improved from 0.83133 to 0.68350, saving model to mymodel_18.h5\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.5776 - mae: 19.9459 - val_loss: 0.6835 - val_mae: 20.0133\n",
            "Epoch 19/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5693 - mae: 19.9475\n",
            "Epoch 00019: val_loss did not improve from 0.68350\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5693 - mae: 19.9476 - val_loss: 0.7372 - val_mae: 20.0252\n",
            "Epoch 20/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5606 - mae: 19.9492\n",
            "Epoch 00020: val_loss did not improve from 0.68350\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.5606 - mae: 19.9498 - val_loss: 0.7735 - val_mae: 20.0323\n",
            "Epoch 21/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5527 - mae: 19.9511\n",
            "Epoch 00021: val_loss did not improve from 0.68350\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5527 - mae: 19.9512 - val_loss: 0.7218 - val_mae: 20.0278\n",
            "Epoch 22/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5459 - mae: 19.9533\n",
            "Epoch 00022: val_loss did not improve from 0.68350\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5459 - mae: 19.9531 - val_loss: 0.7187 - val_mae: 20.0298\n",
            "Epoch 23/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5398 - mae: 19.9554\n",
            "Epoch 00023: val_loss did not improve from 0.68350\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.5398 - mae: 19.9547 - val_loss: 0.7922 - val_mae: 20.0406\n",
            "Epoch 24/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5351 - mae: 19.9567\n",
            "Epoch 00024: val_loss improved from 0.68350 to 0.66954, saving model to mymodel_24.h5\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.5351 - mae: 19.9564 - val_loss: 0.6695 - val_mae: 20.0265\n",
            "Epoch 25/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5291 - mae: 19.9577\n",
            "Epoch 00025: val_loss did not improve from 0.66954\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5291 - mae: 19.9578 - val_loss: 0.7196 - val_mae: 20.0356\n",
            "Epoch 26/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5252 - mae: 19.9592\n",
            "Epoch 00026: val_loss improved from 0.66954 to 0.64931, saving model to mymodel_26.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.5252 - mae: 19.9592 - val_loss: 0.6493 - val_mae: 20.0274\n",
            "Epoch 27/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5207 - mae: 19.9601\n",
            "Epoch 00027: val_loss did not improve from 0.64931\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5206 - mae: 19.9605 - val_loss: 0.6810 - val_mae: 20.0334\n",
            "Epoch 28/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5169 - mae: 19.9626\n",
            "Epoch 00028: val_loss did not improve from 0.64931\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5170 - mae: 19.9617 - val_loss: 0.6684 - val_mae: 20.0334\n",
            "Epoch 29/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5135 - mae: 19.9630\n",
            "Epoch 00029: val_loss did not improve from 0.64931\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.5136 - mae: 19.9629 - val_loss: 0.6572 - val_mae: 20.0333\n",
            "Epoch 30/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5098 - mae: 19.9643\n",
            "Epoch 00030: val_loss did not improve from 0.64931\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.5098 - mae: 19.9640 - val_loss: 0.6782 - val_mae: 20.0372\n",
            "Epoch 31/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5073 - mae: 19.9649\n",
            "Epoch 00031: val_loss improved from 0.64931 to 0.62276, saving model to mymodel_31.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5073 - mae: 19.9653 - val_loss: 0.6228 - val_mae: 20.0306\n",
            "Epoch 32/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5046 - mae: 19.9663\n",
            "Epoch 00032: val_loss improved from 0.62276 to 0.62075, saving model to mymodel_32.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5045 - mae: 19.9661 - val_loss: 0.6207 - val_mae: 20.0316\n",
            "Epoch 33/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.5016 - mae: 19.9679\n",
            "Epoch 00033: val_loss improved from 0.62075 to 0.59907, saving model to mymodel_33.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.5016 - mae: 19.9671 - val_loss: 0.5991 - val_mae: 20.0292\n",
            "Epoch 34/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4997 - mae: 19.9679\n",
            "Epoch 00034: val_loss did not improve from 0.59907\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4997 - mae: 19.9683 - val_loss: 0.6322 - val_mae: 20.0354\n",
            "Epoch 35/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4974 - mae: 19.9688\n",
            "Epoch 00035: val_loss did not improve from 0.59907\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4974 - mae: 19.9689 - val_loss: 0.6276 - val_mae: 20.0360\n",
            "Epoch 36/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4957 - mae: 19.9687\n",
            "Epoch 00036: val_loss did not improve from 0.59907\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4957 - mae: 19.9699 - val_loss: 0.6372 - val_mae: 20.0379\n",
            "Epoch 37/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4937 - mae: 19.9701\n",
            "Epoch 00037: val_loss did not improve from 0.59907\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4937 - mae: 19.9705 - val_loss: 0.6075 - val_mae: 20.0345\n",
            "Epoch 38/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4919 - mae: 19.9714\n",
            "Epoch 00038: val_loss did not improve from 0.59907\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4919 - mae: 19.9712 - val_loss: 0.6583 - val_mae: 20.0423\n",
            "Epoch 39/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4905 - mae: 19.9726\n",
            "Epoch 00039: val_loss improved from 0.59907 to 0.59686, saving model to mymodel_39.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4905 - mae: 19.9720 - val_loss: 0.5969 - val_mae: 20.0346\n",
            "Epoch 40/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4882 - mae: 19.9723\n",
            "Epoch 00040: val_loss improved from 0.59686 to 0.55764, saving model to mymodel_40.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4881 - mae: 19.9726 - val_loss: 0.5576 - val_mae: 20.0282\n",
            "Epoch 41/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4870 - mae: 19.9735\n",
            "Epoch 00041: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4870 - mae: 19.9730 - val_loss: 0.5599 - val_mae: 20.0295\n",
            "Epoch 42/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4854 - mae: 19.9734\n",
            "Epoch 00042: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4854 - mae: 19.9738 - val_loss: 0.6023 - val_mae: 20.0375\n",
            "Epoch 43/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4844 - mae: 19.9730\n",
            "Epoch 00043: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4844 - mae: 19.9743 - val_loss: 0.5733 - val_mae: 20.0335\n",
            "Epoch 44/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4832 - mae: 19.9754\n",
            "Epoch 00044: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4832 - mae: 19.9748 - val_loss: 0.5674 - val_mae: 20.0330\n",
            "Epoch 45/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4819 - mae: 19.9755\n",
            "Epoch 00045: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4819 - mae: 19.9754 - val_loss: 0.5799 - val_mae: 20.0357\n",
            "Epoch 46/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4808 - mae: 19.9748\n",
            "Epoch 00046: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4808 - mae: 19.9757 - val_loss: 0.5642 - val_mae: 20.0338\n",
            "Epoch 47/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4792 - mae: 19.9766\n",
            "Epoch 00047: val_loss did not improve from 0.55764\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4793 - mae: 19.9762 - val_loss: 0.6975 - val_mae: 20.0514\n",
            "Epoch 48/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4799 - mae: 19.9773\n",
            "Epoch 00048: val_loss improved from 0.55764 to 0.54896, saving model to mymodel_48.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4799 - mae: 19.9766 - val_loss: 0.5490 - val_mae: 20.0318\n",
            "Epoch 49/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4779 - mae: 19.9769\n",
            "Epoch 00049: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4779 - mae: 19.9769 - val_loss: 0.5779 - val_mae: 20.0375\n",
            "Epoch 50/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4764 - mae: 19.9769\n",
            "Epoch 00050: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4764 - mae: 19.9774 - val_loss: 0.5617 - val_mae: 20.0350\n",
            "Epoch 51/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4760 - mae: 19.9777\n",
            "Epoch 00051: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4760 - mae: 19.9777 - val_loss: 0.5699 - val_mae: 20.0372\n",
            "Epoch 52/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4748 - mae: 19.9780\n",
            "Epoch 00052: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4748 - mae: 19.9780 - val_loss: 0.5654 - val_mae: 20.0369\n",
            "Epoch 53/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4739 - mae: 19.9788\n",
            "Epoch 00053: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4739 - mae: 19.9785 - val_loss: 0.5739 - val_mae: 20.0386\n",
            "Epoch 54/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4730 - mae: 19.9783\n",
            "Epoch 00054: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4730 - mae: 19.9788 - val_loss: 0.5782 - val_mae: 20.0395\n",
            "Epoch 55/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4723 - mae: 19.9795\n",
            "Epoch 00055: val_loss did not improve from 0.54896\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4723 - mae: 19.9790 - val_loss: 0.5572 - val_mae: 20.0366\n",
            "Epoch 56/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4717 - mae: 19.9805\n",
            "Epoch 00056: val_loss improved from 0.54896 to 0.52663, saving model to mymodel_56.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4718 - mae: 19.9795 - val_loss: 0.5266 - val_mae: 20.0306\n",
            "Epoch 57/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4714 - mae: 19.9786\n",
            "Epoch 00057: val_loss did not improve from 0.52663\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4714 - mae: 19.9794 - val_loss: 0.5543 - val_mae: 20.0368\n",
            "Epoch 58/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4699 - mae: 19.9800\n",
            "Epoch 00058: val_loss did not improve from 0.52663\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4698 - mae: 19.9799 - val_loss: 0.5443 - val_mae: 20.0350\n",
            "Epoch 59/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4695 - mae: 19.9789\n",
            "Epoch 00059: val_loss did not improve from 0.52663\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4695 - mae: 19.9801 - val_loss: 0.5276 - val_mae: 20.0321\n",
            "Epoch 60/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4689 - mae: 19.9799\n",
            "Epoch 00060: val_loss did not improve from 0.52663\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4689 - mae: 19.9803 - val_loss: 0.5511 - val_mae: 20.0375\n",
            "Epoch 61/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4685 - mae: 19.9796\n",
            "Epoch 00061: val_loss improved from 0.52663 to 0.51602, saving model to mymodel_61.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4685 - mae: 19.9806 - val_loss: 0.5160 - val_mae: 20.0302\n",
            "Epoch 62/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4679 - mae: 19.9810\n",
            "Epoch 00062: val_loss did not improve from 0.51602\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4679 - mae: 19.9809 - val_loss: 0.5325 - val_mae: 20.0341\n",
            "Epoch 63/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4673 - mae: 19.9816\n",
            "Epoch 00063: val_loss did not improve from 0.51602\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4673 - mae: 19.9809 - val_loss: 0.5282 - val_mae: 20.0337\n",
            "Epoch 64/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4667 - mae: 19.9814\n",
            "Epoch 00064: val_loss did not improve from 0.51602\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4667 - mae: 19.9813 - val_loss: 0.5321 - val_mae: 20.0345\n",
            "Epoch 65/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4662 - mae: 19.9809\n",
            "Epoch 00065: val_loss did not improve from 0.51602\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4662 - mae: 19.9814 - val_loss: 0.5410 - val_mae: 20.0367\n",
            "Epoch 66/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4659 - mae: 19.9818\n",
            "Epoch 00066: val_loss improved from 0.51602 to 0.51332, saving model to mymodel_66.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4659 - mae: 19.9816 - val_loss: 0.5133 - val_mae: 20.0310\n",
            "Epoch 67/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4651 - mae: 19.9811\n",
            "Epoch 00067: val_loss did not improve from 0.51332\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4651 - mae: 19.9819 - val_loss: 0.5568 - val_mae: 20.0399\n",
            "Epoch 68/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4646 - mae: 19.9821\n",
            "Epoch 00068: val_loss improved from 0.51332 to 0.50655, saving model to mymodel_68.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4647 - mae: 19.9821 - val_loss: 0.5066 - val_mae: 20.0300\n",
            "Epoch 69/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4645 - mae: 19.9822\n",
            "Epoch 00069: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4645 - mae: 19.9821 - val_loss: 0.5175 - val_mae: 20.0331\n",
            "Epoch 70/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4637 - mae: 19.9824\n",
            "Epoch 00070: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4636 - mae: 19.9826 - val_loss: 0.5457 - val_mae: 20.0389\n",
            "Epoch 71/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4634 - mae: 19.9826\n",
            "Epoch 00071: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4634 - mae: 19.9827 - val_loss: 0.5179 - val_mae: 20.0334\n",
            "Epoch 72/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4625 - mae: 19.9827\n",
            "Epoch 00072: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4626 - mae: 19.9827 - val_loss: 0.5426 - val_mae: 20.0390\n",
            "Epoch 73/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4626 - mae: 19.9823\n",
            "Epoch 00073: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4626 - mae: 19.9830 - val_loss: 0.5412 - val_mae: 20.0388\n",
            "Epoch 74/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4617 - mae: 19.9839\n",
            "Epoch 00074: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4617 - mae: 19.9831 - val_loss: 0.5223 - val_mae: 20.0354\n",
            "Epoch 75/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4618 - mae: 19.9827\n",
            "Epoch 00075: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4618 - mae: 19.9834 - val_loss: 0.5345 - val_mae: 20.0380\n",
            "Epoch 76/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4611 - mae: 19.9836\n",
            "Epoch 00076: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4611 - mae: 19.9835 - val_loss: 0.5633 - val_mae: 20.0429\n",
            "Epoch 77/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4614 - mae: 19.9834\n",
            "Epoch 00077: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4614 - mae: 19.9838 - val_loss: 0.5297 - val_mae: 20.0376\n",
            "Epoch 78/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4604 - mae: 19.9841\n",
            "Epoch 00078: val_loss did not improve from 0.50655\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4605 - mae: 19.9838 - val_loss: 0.5354 - val_mae: 20.0388\n",
            "Epoch 79/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4602 - mae: 19.9834\n",
            "Epoch 00079: val_loss improved from 0.50655 to 0.50035, saving model to mymodel_79.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4602 - mae: 19.9839 - val_loss: 0.5003 - val_mae: 20.0312\n",
            "Epoch 80/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4603 - mae: 19.9838\n",
            "Epoch 00080: val_loss did not improve from 0.50035\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4603 - mae: 19.9840 - val_loss: 0.5034 - val_mae: 20.0319\n",
            "Epoch 81/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4598 - mae: 19.9832\n",
            "Epoch 00081: val_loss did not improve from 0.50035\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4598 - mae: 19.9841 - val_loss: 0.5312 - val_mae: 20.0385\n",
            "Epoch 82/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4591 - mae: 19.9832\n",
            "Epoch 00082: val_loss did not improve from 0.50035\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4591 - mae: 19.9844 - val_loss: 0.5257 - val_mae: 20.0374\n",
            "Epoch 83/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4591 - mae: 19.9837\n",
            "Epoch 00083: val_loss did not improve from 0.50035\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4590 - mae: 19.9843 - val_loss: 0.5011 - val_mae: 20.0321\n",
            "Epoch 84/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4585 - mae: 19.9851\n",
            "Epoch 00084: val_loss did not improve from 0.50035\n",
            "364/364 [==============================] - 25s 69ms/step - loss: 0.4585 - mae: 19.9845 - val_loss: 0.5154 - val_mae: 20.0359\n",
            "Epoch 85/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4586 - mae: 19.9845\n",
            "Epoch 00085: val_loss improved from 0.50035 to 0.49725, saving model to mymodel_85.h5\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4586 - mae: 19.9847 - val_loss: 0.4973 - val_mae: 20.0314\n",
            "Epoch 86/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4580 - mae: 19.9854\n",
            "Epoch 00086: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4580 - mae: 19.9849 - val_loss: 0.4985 - val_mae: 20.0319\n",
            "Epoch 87/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4582 - mae: 19.9848\n",
            "Epoch 00087: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4582 - mae: 19.9848 - val_loss: 0.5009 - val_mae: 20.0328\n",
            "Epoch 88/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4575 - mae: 19.9862\n",
            "Epoch 00088: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4575 - mae: 19.9851 - val_loss: 0.5033 - val_mae: 20.0333\n",
            "Epoch 89/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4576 - mae: 19.9856\n",
            "Epoch 00089: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4576 - mae: 19.9849 - val_loss: 0.5202 - val_mae: 20.0377\n",
            "Epoch 90/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4571 - mae: 19.9840\n",
            "Epoch 00090: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4571 - mae: 19.9854 - val_loss: 0.5199 - val_mae: 20.0376\n",
            "Epoch 91/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4572 - mae: 19.9843\n",
            "Epoch 00091: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4571 - mae: 19.9854 - val_loss: 0.5339 - val_mae: 20.0402\n",
            "Epoch 92/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4562 - mae: 19.9855\n",
            "Epoch 00092: val_loss did not improve from 0.49725\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4562 - mae: 19.9852 - val_loss: 0.4975 - val_mae: 20.0328\n",
            "Epoch 93/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4563 - mae: 19.9851\n",
            "Epoch 00093: val_loss improved from 0.49725 to 0.49026, saving model to mymodel_93.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4563 - mae: 19.9857 - val_loss: 0.4903 - val_mae: 20.0303\n",
            "Epoch 94/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4569 - mae: 19.9832\n",
            "Epoch 00094: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4569 - mae: 19.9854 - val_loss: 0.4916 - val_mae: 20.0309\n",
            "Epoch 95/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4556 - mae: 19.9860\n",
            "Epoch 00095: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4556 - mae: 19.9856 - val_loss: 0.5415 - val_mae: 20.0422\n",
            "Epoch 96/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4559 - mae: 19.9857\n",
            "Epoch 00096: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4559 - mae: 19.9857 - val_loss: 0.5163 - val_mae: 20.0377\n",
            "Epoch 97/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4554 - mae: 19.9871\n",
            "Epoch 00097: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4555 - mae: 19.9859 - val_loss: 0.5165 - val_mae: 20.0376\n",
            "Epoch 98/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4554 - mae: 19.9860\n",
            "Epoch 00098: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4555 - mae: 19.9859 - val_loss: 0.5371 - val_mae: 20.0419\n",
            "Epoch 99/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4550 - mae: 19.9865\n",
            "Epoch 00099: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4550 - mae: 19.9860 - val_loss: 0.5275 - val_mae: 20.0400\n",
            "Epoch 100/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4549 - mae: 19.9861\n",
            "Epoch 00100: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4549 - mae: 19.9860 - val_loss: 0.5195 - val_mae: 20.0388\n",
            "Epoch 101/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4547 - mae: 19.9857\n",
            "Epoch 00101: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4547 - mae: 19.9861 - val_loss: 0.5280 - val_mae: 20.0402\n",
            "Epoch 102/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4548 - mae: 19.9857\n",
            "Epoch 00102: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4548 - mae: 19.9862 - val_loss: 0.5064 - val_mae: 20.0361\n",
            "Epoch 103/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4544 - mae: 19.9872\n",
            "Epoch 00103: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4544 - mae: 19.9864 - val_loss: 0.5264 - val_mae: 20.0402\n",
            "Epoch 104/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4536 - mae: 19.9854\n",
            "Epoch 00104: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4536 - mae: 19.9864 - val_loss: 0.5006 - val_mae: 20.0347\n",
            "Epoch 105/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4536 - mae: 19.9865\n",
            "Epoch 00105: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4536 - mae: 19.9865 - val_loss: 0.5014 - val_mae: 20.0350\n",
            "Epoch 106/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4534 - mae: 19.9858\n",
            "Epoch 00106: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4535 - mae: 19.9864 - val_loss: 0.5389 - val_mae: 20.0429\n",
            "Epoch 107/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4535 - mae: 19.9878\n",
            "Epoch 00107: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4535 - mae: 19.9867 - val_loss: 0.5476 - val_mae: 20.0446\n",
            "Epoch 108/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4538 - mae: 19.9860\n",
            "Epoch 00108: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4537 - mae: 19.9868 - val_loss: 0.4987 - val_mae: 20.0346\n",
            "Epoch 109/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4529 - mae: 19.9869\n",
            "Epoch 00109: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4528 - mae: 19.9867 - val_loss: 0.5023 - val_mae: 20.0359\n",
            "Epoch 110/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4530 - mae: 19.9864\n",
            "Epoch 00110: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4530 - mae: 19.9869 - val_loss: 0.5067 - val_mae: 20.0370\n",
            "Epoch 111/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4529 - mae: 19.9868\n",
            "Epoch 00111: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4528 - mae: 19.9869 - val_loss: 0.4980 - val_mae: 20.0352\n",
            "Epoch 112/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4524 - mae: 19.9872\n",
            "Epoch 00112: val_loss did not improve from 0.49026\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4524 - mae: 19.9869 - val_loss: 0.5349 - val_mae: 20.0427\n",
            "Epoch 113/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4531 - mae: 19.9877\n",
            "Epoch 00113: val_loss improved from 0.49026 to 0.48942, saving model to mymodel_113.h5\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4531 - mae: 19.9871 - val_loss: 0.4894 - val_mae: 20.0328\n",
            "Epoch 114/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4525 - mae: 19.9871\n",
            "Epoch 00114: val_loss did not improve from 0.48942\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4525 - mae: 19.9872 - val_loss: 0.4924 - val_mae: 20.0339\n",
            "Epoch 115/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4522 - mae: 19.9861\n",
            "Epoch 00115: val_loss improved from 0.48942 to 0.48512, saving model to mymodel_115.h5\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4522 - mae: 19.9871 - val_loss: 0.4851 - val_mae: 20.0318\n",
            "Epoch 116/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4521 - mae: 19.9865\n",
            "Epoch 00116: val_loss did not improve from 0.48512\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4521 - mae: 19.9872 - val_loss: 0.5012 - val_mae: 20.0360\n",
            "Epoch 117/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4521 - mae: 19.9873\n",
            "Epoch 00117: val_loss did not improve from 0.48512\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4521 - mae: 19.9872 - val_loss: 0.4943 - val_mae: 20.0347\n",
            "Epoch 118/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4514 - mae: 19.9872\n",
            "Epoch 00118: val_loss did not improve from 0.48512\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4514 - mae: 19.9874 - val_loss: 0.5111 - val_mae: 20.0386\n",
            "Epoch 119/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4513 - mae: 19.9871\n",
            "Epoch 00119: val_loss did not improve from 0.48512\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4512 - mae: 19.9873 - val_loss: 0.5527 - val_mae: 20.0460\n",
            "Epoch 120/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4521 - mae: 19.9872\n",
            "Epoch 00120: val_loss improved from 0.48512 to 0.47068, saving model to mymodel_120.h5\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4521 - mae: 19.9875 - val_loss: 0.4707 - val_mae: 20.0265\n",
            "Epoch 121/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4515 - mae: 19.9870\n",
            "Epoch 00121: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4515 - mae: 19.9874 - val_loss: 0.5049 - val_mae: 20.0377\n",
            "Epoch 122/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4514 - mae: 19.9879\n",
            "Epoch 00122: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4514 - mae: 19.9876 - val_loss: 0.4783 - val_mae: 20.0302\n",
            "Epoch 123/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4508 - mae: 19.9872\n",
            "Epoch 00123: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4508 - mae: 19.9876 - val_loss: 0.5047 - val_mae: 20.0377\n",
            "Epoch 124/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4509 - mae: 19.9885\n",
            "Epoch 00124: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4509 - mae: 19.9876 - val_loss: 0.5162 - val_mae: 20.0400\n",
            "Epoch 125/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4506 - mae: 19.9873\n",
            "Epoch 00125: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4506 - mae: 19.9877 - val_loss: 0.5015 - val_mae: 20.0373\n",
            "Epoch 126/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4502 - mae: 19.9877\n",
            "Epoch 00126: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4502 - mae: 19.9877 - val_loss: 0.5061 - val_mae: 20.0380\n",
            "Epoch 127/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4502 - mae: 19.9883\n",
            "Epoch 00127: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4502 - mae: 19.9879 - val_loss: 0.4986 - val_mae: 20.0362\n",
            "Epoch 128/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4504 - mae: 19.9880\n",
            "Epoch 00128: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4504 - mae: 19.9877 - val_loss: 0.4950 - val_mae: 20.0359\n",
            "Epoch 129/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4500 - mae: 19.9889\n",
            "Epoch 00129: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4500 - mae: 19.9880 - val_loss: 0.4892 - val_mae: 20.0341\n",
            "Epoch 130/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4498 - mae: 19.9889\n",
            "Epoch 00130: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4498 - mae: 19.9879 - val_loss: 0.5086 - val_mae: 20.0389\n",
            "Epoch 131/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4501 - mae: 19.9879\n",
            "Epoch 00131: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4501 - mae: 19.9879 - val_loss: 0.4744 - val_mae: 20.0295\n",
            "Epoch 132/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4499 - mae: 19.9878\n",
            "Epoch 00132: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4499 - mae: 19.9878 - val_loss: 0.5064 - val_mae: 20.0389\n",
            "Epoch 133/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4497 - mae: 19.9883\n",
            "Epoch 00133: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 25s 70ms/step - loss: 0.4497 - mae: 19.9882 - val_loss: 0.4930 - val_mae: 20.0352\n",
            "Epoch 134/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4492 - mae: 19.9875\n",
            "Epoch 00134: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4492 - mae: 19.9880 - val_loss: 0.5121 - val_mae: 20.0404\n",
            "Epoch 135/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4492 - mae: 19.9882\n",
            "Epoch 00135: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4491 - mae: 19.9883 - val_loss: 0.4858 - val_mae: 20.0337\n",
            "Epoch 136/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4492 - mae: 19.9878\n",
            "Epoch 00136: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4492 - mae: 19.9881 - val_loss: 0.5404 - val_mae: 20.0451\n",
            "Epoch 137/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4494 - mae: 19.9877\n",
            "Epoch 00137: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4494 - mae: 19.9883 - val_loss: 0.4976 - val_mae: 20.0369\n",
            "Epoch 138/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4488 - mae: 19.9881\n",
            "Epoch 00138: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4488 - mae: 19.9882 - val_loss: 0.4946 - val_mae: 20.0362\n",
            "Epoch 139/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4491 - mae: 19.9880\n",
            "Epoch 00139: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4491 - mae: 19.9884 - val_loss: 0.5120 - val_mae: 20.0399\n",
            "Epoch 140/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4490 - mae: 19.9887\n",
            "Epoch 00140: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4490 - mae: 19.9884 - val_loss: 0.4852 - val_mae: 20.0336\n",
            "Epoch 141/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4485 - mae: 19.9891\n",
            "Epoch 00141: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4485 - mae: 19.9886 - val_loss: 0.4879 - val_mae: 20.0341\n",
            "Epoch 142/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4485 - mae: 19.9879\n",
            "Epoch 00142: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4485 - mae: 19.9883 - val_loss: 0.4901 - val_mae: 20.0353\n",
            "Epoch 143/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4484 - mae: 19.9886\n",
            "Epoch 00143: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4484 - mae: 19.9884 - val_loss: 0.5096 - val_mae: 20.0398\n",
            "Epoch 144/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4484 - mae: 19.9885\n",
            "Epoch 00144: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 70ms/step - loss: 0.4484 - mae: 19.9885 - val_loss: 0.5031 - val_mae: 20.0386\n",
            "Epoch 145/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4484 - mae: 19.9879\n",
            "Epoch 00145: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4484 - mae: 19.9885 - val_loss: 0.4877 - val_mae: 20.0349\n",
            "Epoch 146/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4482 - mae: 19.9885\n",
            "Epoch 00146: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4482 - mae: 19.9886 - val_loss: 0.4868 - val_mae: 20.0346\n",
            "Epoch 147/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4477 - mae: 19.9883\n",
            "Epoch 00147: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4477 - mae: 19.9884 - val_loss: 0.4916 - val_mae: 20.0361\n",
            "Epoch 148/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4477 - mae: 19.9889\n",
            "Epoch 00148: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4477 - mae: 19.9886 - val_loss: 0.4915 - val_mae: 20.0359\n",
            "Epoch 149/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4479 - mae: 19.9893\n",
            "Epoch 00149: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4479 - mae: 19.9887 - val_loss: 0.4884 - val_mae: 20.0353\n",
            "Epoch 150/200\n",
            "363/364 [============================>.] - ETA: 0s - loss: 0.4478 - mae: 19.9889\n",
            "Epoch 00150: val_loss did not improve from 0.47068\n",
            "364/364 [==============================] - 26s 71ms/step - loss: 0.4478 - mae: 19.9888 - val_loss: 0.5069 - val_mae: 20.0394\n",
            "dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODupPHGFeks",
        "outputId": "62e52348-90c7-42b9-a1b9-402eda490054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Save the best model\n",
        "shutil.copy('/content/mymodel_120.h5', '/content/drive/My Drive/mymodel_120.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/mymodel_50.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}